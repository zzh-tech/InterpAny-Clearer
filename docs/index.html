<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame Interpolation.">
    <meta name="keywords" content="video frame interpolation, anytime, clearer, manipulated interpolation of anything">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>InterpAny-Clearer</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-EDF010G6PN"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-EDF010G6PN');


    </script>

    <script type="module"
            src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
    <script src="https://unpkg.com/interactjs/dist/interact.min.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="./static/slick/slick.css"/>
    <link rel="stylesheet" type="text/css" href="./static/slick/slick-theme.css"/>

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
<!--    <link rel="icon" href="./static/images/logo.png">-->

    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
            <a class="navbar-item" href="https://zzh-tech.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
            </a>

            <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    More Research
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="https://zzh-tech.github.io/BiT/">
                        BiT
                    </a>
                    <a class="navbar-item" href="https://zzh-tech.github.io/Animation-from-Blur/">
                        Animation-from-Blur
                    </a>
                    <a class="navbar-item" href="https://zzh-tech.github.io/Dual-Reversed-RS/">
                        Dual-Reversed-RS
                    </a>
                </div>
            </div>
        </div>

    </div>
</nav>


<section class="hero">
    <div class="hero-body">
        <div class="container">
            <!--            <div class="columns is-centered">-->
            <!--                <div class="column is-4 has-text-centered">-->
            <!--                    <img src="static/images/logo.png" alt="InterpAny-Clearer logo" width="288" style="border-radius: 8%;"/>-->
            <!--                </div>-->
            <!--            </div>-->
            <br>
            <div class="container has-text-centered">
                <h1 class="title is-1 publication-title">
                    <u style="color: hotpink; text-decoration: underline dotted hotpink;">Clearer Frames,</u>
                    <u style="color: dodgerblue; text-decoration: underline dotted dodgerblue;">Anytime:</u> Resolving
                    Velocity Ambiguity in Video Frame
                    Interpolation </h1>
                <div class="is-size-5 publication-authors">
                        <span class="author-block"><a
                                href="https://zzh-tech.github.io/">Zhihang Zhong</a><sup>1,*</sup>,  </span>
                    <span class="author-block"><a
                            href="https://scholar.google.com/citations?user=BKYVv4MAAAAJ&hl=en">Gurunandan Krishnan</a><sup>2</sup>,  </span>
                    <span class="author-block"><a
                            href="https://jimmysuen.github.io/">Xiao Sun</a><sup>1</sup>,  </span>
                    <span class="author-block"><a
                            href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en">Yu Qiao</a><sup>1</sup>,  </span>
                    <span class="author-block"><a
                            href="https://sizhuoma.netlify.app/">Sizhuo Ma</a><sup>2,†</sup>,  </span>
                    <span class="author-block"><a
                            href="https://jianwang-cmu.github.io/">Jian Wang</a><sup>2,†</sup></span>
                </div>

                <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup><a href="https://github.com/OpenGVLab">Shanghai AI Laboratory, OpenGVLab</a>, </span>
                    <span class="author-block"><sup>2</sup><a href="https://snap.com/en-US">Snap Inc.</a></span>
                    <br>
                    <span class="author-block"><sup>*</sup>First author,</span>
                    <span class="author-block"><sup>†</sup>Co-corresponding authors</span>
                </div>

                <div class="column has-text-centered">
                    <div class="publication-links">
                        <!--                        <span class="link-block">-->
                        <!--                            <a href="https://github.com/zzh-tech/InterpAny-Clearer"-->
                        <!--                               class="external-link button is-normal is-rounded is-dark">-->
                        <!--                              <span class="icon">-->
                        <!--                                  <i class="fas fa-file-pdf"></i>-->
                        <!--                              </span>-->
                        <!--                              <span>Paper</span>-->
                        <!--                            </a>-->
                        <!--                          </span>-->
                        <span class="link-block">
                            <a href="http://arxiv.org/abs/2311.08007"
                               class="external-link button is-normal is-rounded is-dark"
                               target="_blank">
                              <span class="icon">
                                  <i class="ai ai-arxiv"></i>
                              </span>
                              <span>arXiv</span>
                            </a>
                        </span>
                        <span class="link-block">
                          <a href="https://github.com/zzh-tech/InterpAny-Clearer"
                             class="external-link button is-normal is-rounded is-dark"
                             target="_blank">
                            <span class="icon">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                            </a>
                        </span>
                        <span class="link-block">
                          <a href="http://ai4sports.opengvlab.com/interpany-clearer/"
                             class="external-link button is-normal is-rounded is-dark"
                             target="_blank">
                            <span class="icon">
                                <i class="fa fa-star"></i>
                            </span>
                            <span>Demo</span>
                            </a>
                        </span>
                        <div class="is-size-5 publication-authors">
                            <img src="./static/images/eccv-navbar-logo.svg"
                                 style="width: 18%;text-align: center">
                            <br>
                            <span style="color:darkred"><b>Accepted to ECCV'2024 as Oral</b></span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <img src="./static/images/teaser.jpg" alt="teaser">
            <br>
            <br>
            <h2 class="subtitle has-text-centered">
                <b>
                    We addressed velocity ambiguity in video frame interpolation, leading to: <br>
                    <em style="color: red">Clearer anytime frame interpolation</em> & <em style="color: red">Manipulated
                    interpolation of anything</em>.
                </b>
            </h2>
        </div>
    </div>
</section>

<br>

<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2 class="title is-3">Clearer anytime frame interpolation</h2>
    </div>
</div>
<br>

<section class="hero is-dark is-small">
    <div class="hero-body">
        <div class="container has-text-centered">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="results-item">
                    <video poster="" id="00006/0277" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/webapp_demo/00006/0277_demo_converted.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="results-item">
                    <video poster="" id="00029/0419" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/webapp_demo/00029/0419_demo_converted.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="results-item">
                    <video poster="" id="00036/0310" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/webapp_demo/00036/0310_demo_converted.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="results-item">
                    <video poster="" id="00036/0320" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/webapp_demo/00036/0320_demo_converted.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="results-item">
                    <video poster="" id="00037/0345" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/webapp_demo/00037/0345_demo_converted.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="results-item">
                    <video poster="" id="00068/0261" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/webapp_demo/00068/0261_demo_converted.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="results-item">
                    <video poster="" id="00071/0366" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/webapp_demo/00071/0366_demo_converted.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="results-item">
                    <video poster="" id="00071/0808" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/webapp_demo/00071/0808_demo_converted.mp4"
                                type="video/mp4">
                    </video>
                </div>

                <div class="results-item">
                    <video poster="" id="00080/0050" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/webapp_demo/00080/0050_demo_converted.mp4"
                                type="video/mp4">
                    </video>
                </div>

            </div>

            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <p>
                        <em>
                            <b>When integrating our plug-and-play training strategies ([D,R]) into the state-of-the-art
                                learning-based models such as
                                <a href="https://github.com/megvii-research/ECCV2022-RIFE">RIFE [1]</a>,
                                <a href="https://github.com/ltkong218/IFRNet">IFRNet [2]</a>,
                                <a href="https://github.com/MCG-NKU/AMT">AMT [3]</a>, and
                                <a href="https://github.com/ltkong218/IFRNet">EMA-VFI [4]</a>, they exhibit markedly
                                sharper outputs and superior perceptual quality in arbitrary time interpolations. <br>
                                (Here, we employ RIFE as an illustrative example, generating 128 interpolated frames
                                using just two images.)
                            </b>
                        </em>
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<br>
<br>

<section class="section">
    <!-- Paper video. -->
    <div style="display: flex; justify-content: center; align-items: center; height: 75vh; width: 100%;">
        <div style="width: 66%; height: auto;">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Application in CCTV</h2>
                    <br>
                    <div class="publication-video">
                        <iframe src="./static/webapp_demo/cctv5_interpany-clearer.mp4" encrypted-media allowfullscreen></iframe>
                    </div>
                    <br>
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-three-quarters">
                            <p>
                                <em>
                                    <b>
                                        <u style="color: black; text-decoration: underline black;">
                                            Our technology was used by CCTV5 as well as CCTV5+ for slow motion
                                            demonstrations of athletes jumping in the 2024 Thomas & Uber Cup.</u>
                                    </b>
                                </em>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!--/ Paper video. -->
</section>

<br>
<br>

<section class="section">
    <!-- Paper video. -->
    <div style="display: flex; justify-content: center; align-items: center; height: 75vh; width: 100%;">
        <div style="width: 66%; height: auto;">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Manipulated interpolation of anything</h2>
                    <br>
                    <div class="publication-video">
                        <iframe src="./static/webapp_demo/InterpAny-Clearer-Demo.mp4"
                                allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </div>
                    <br>
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-three-quarters">
                            <p>
                                <em>
                                    <b>
                                        Additionally, our strategies enable temporal manipulation of each object
                                        independently during the inference stage, offering a novel tool for video
                                        editing tasks like re-timing. <u
                                            style="color: black; text-decoration: underline black;">Our <a
                                            href="http://ai4sports.opengvlab.com/interpany-clearer/"
                                            target="_blank">APP</a> also
                                        supports video as an input to classic video frame interpolation.</u>
                                    </b>
                                </em>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!--/ Paper video. -->
</section>

<hr/>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Existing video frame interpolation (VFI) methods blindly predict where each object is at a
                        specific timestep t ("time indexing"), which struggles to predict precise object movements.
                        Given two images of a &#9918, there are infinitely many possible trajectories: accelerating or
                        decelerating, straight or curved. This often results in blurry frames as the method averages out
                        these possibilities. Instead of forcing the network to learn this complicated time-to-location
                        mapping implicitly together with predicting the frames, we provide the network with an explicit
                        hint on how far the object has traveled between start and end frames, a novel approach termed
                        "distance indexing". This method offers a clearer learning goal for models, reducing the
                        uncertainty tied to object speeds. We further observed that, even with this extra guidance,
                        objects can still be blurry especially when they are equally far from both input frames
                        (<i>i.e.</i>, halfway in-between), due to the directional ambiguity in long-range motion. To
                        solve this, we propose an iterative reference-based estimation strategy that breaks down a
                        long-range prediction into several short-range steps. When integrating our plug-and-play
                        strategies into state-of-the-art learning-based models, they exhibit markedly sharper outputs
                        and superior perceptual quality in arbitrary time interpolations, using a uniform distance
                        indexing map in the same format as time indexing. Additionally, distance indexing can be
                        specified pixel-wise, which enables temporal manipulation of each object independently, offering
                        a novel tool for video editing tasks like re-timing.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->

        <!--    &lt;!&ndash; Paper video. &ndash;&gt;-->
        <!--    <div class="columns is-centered has-text-centered">-->
        <!--      <div class="column is-four-fifths">-->
        <!--        <h2 class="title is-2">Video</h2>-->
        <!--        <div class="publication-video">-->
        <!--          <iframe width="640" height="480" src="https://www.youtube.com/embed/qzgdE_ghkaI"-->
        <!--                  title="YouTube video player" frameborder="0"-->
        <!--                  allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture"-->
        <!--                  allowfullscreen></iframe>-->
        <!--        </div>-->
        <!--      </div>-->
        <!--    </div>-->
        <!--    &lt;!&ndash;/ Paper video. &ndash;&gt;-->
    </div>
</section>

<section>
    <div class="container is-max-desktop">
        <div class="content has-text-justified">
            <h3 class="title is-4">Velocity ambiguity problem</h3>
            <p>
                The current mainstream algorithm for arbitrary-time frame interpolation, denoted as \(\mathcal{F}\),
                predicts the target frame \(I_t\) using the starting frame \(I_0\), the ending frame \(I_1\), and a time
                index
                \(t\) as inputs:

                $$I_t = \mathcal{F}\left(I_0, I_1, t\right)$$

                However, the unknown motion velocity of each independently moving object introduces the issue of
                <b>"velocity ambiguity"</b>. This means there are multiple possible mappings from the same inputs to
                different locations:

                $$\left\{I_t^1, I_t^2, \ldots, I_t^n\right\} = \mathcal{F}(I_0, I_1, t)$$

                Taking a &#9918 as an example, there are countless potential landing spots for the ball in mid-air,
                leading to conflicts in learning during the training process.

                In short, the algorithm cannot discern which scenario is correct to learn, so it settles for an average
                state:

                $$ \hat{I}_t = \mathbb{E}_{I_t \sim \mathcal{F}(I_0, I_1, t)}[I_t] $$

                This results in the algorithm's prediction \(\hat{I}_t\) being blurry during testing:
            </p>
            <div class="has-text-centered">
                <img style="width: 100%;" src="./static/images/velocity_ambiguity.jpg"
                     alt="Velocity ambiguity"/>
            </div>
        </div>
    </div>
</section>

<br>
<br>

<section>
    <div class="container is-max-desktop">
        <div class="content has-text-justified">
            <h3 class="title is-4">Disambiguation</h3>
            <p>
                To resolve velocity ambiguity, a new paradigm for index-based learning is required.
                <i><b>We need to guide the algorithm on why objects land in certain positions:</b></i>
                $$I_{t} = \mathcal{F}\left(I_0, I_1, \text{motion hint}\right)$$
            </p>
            <h4 class="title is-5">Distance indexing</h4>
            <p>
                In fact, the training approach using time indexing requires the algorithm to not only learn how to
                interpolate frames but also to guess the mapping relationship from time to position, denoted as
                \(\mathcal{D}\):

                $$I_t = \mathcal{F}(I_0, I_1, t) \to I_t = \mathcal{F}(I_0, I_1, \mathcal{D}(t))$$

                Our solution involves calculating a path distance ratio map \(D_t\) to replace the time \(t\) for
                index-based learning:

                $$I_t = \mathcal{F}(I_0, I_1, \mathcal{D}(t)) \to I_t = \mathcal{F}(I_0, I_1, D_t)$$
            </p>
            <div class="has-text-centered">
                <img style="width: 100%;" src="./static/images/distance_indexing.jpg"
                     alt="Distance indexing"/>
            </div>
            <br>
            <p>
                We first compute the optical flows from \(I_0\) to \(I_t\) and from \(I_0\) to \(I_1\), denoted as
                \(V_{0\to t}\) and \(V_{0\to 1}\), respectively. Then, for each pixel location \((x,y)\), we calculate
                the proportion of the optical flow \(V_{0\to t}\) projected onto \(V_{0\to 1}\), denoted as the "path
                distance ratio":

                $$D_t(x,y) = \frac{\left\Vert \mathbf{V}_{0\to t}(x,y)\right\Vert \cos{\theta}}{\left\Vert
                \mathbf{V}_{0\to 1}(x,y) \right\Vert}$$

                <i><b>With \(D_t\), the algorithm avoids the ambiguous time-to-position mappings during training caused
                    by varying velocities, allowing for clearer predictions during testing. Importantly, even without
                    the ability to calculate the exact \(D_t\) using ground truth labels during inference, providing a
                    uniform index map similar to time indexing, <i>i.e.</i>, \(D_t=t\), the algorithm can still predict
                    clearer images (simulating uniform motion of objects).</b></i>
            </p>
            <br>
            <h4 class="title is-5">Iterative reference-based estimation</h4>
            <p>
                While path distance indexing helps us sidestep ambiguities in speed, it does not resolve directional
                ambiguities. We apply a classic divide-and-conquer strategy to minimize the impact of directional
                ambiguities, further improving the predicted outcome.
                <br>
                <b>In practice, we break down a long-distance inference into a series of short-distance inferences from
                    near to far, using the previous inference, along with the starting and ending frames, as references
                    to avoid accumulated errors:</b>

                $$I_t = \mathcal{F}(I_0, I_1, D_t, I_{\text{ref}}, D_{\text{ref}})$$

                For example, dividing the inference into two steps would look like:

                $$I_{t/2} = \mathcal{F}(I_0, I_1, D_{t/2}, I_{0}, D_{0})$$

                $$I_{t} = \mathcal{F}(I_0, I_1, D_{t}, I_{t/2}, D_{t/2})$$

                Similarly, taking a &#9918 as an example, our proposed strategies are illustrated in the following
                figure:
            </p>
            <div class="has-text-centered">
                <img style="width: 100%;" src="./static/images/disambiguation.jpg"
                     alt="Disambiguation"/>
            </div>
            <br>
            <br>
            <h4 class="title is-5">Editable interpolation</h4>
            <p>
                <b>Beyond using a uniform index map like time indexing, we can also take advantage of the 2D editable
                    nature of path distance indexing to implement editable frame interpolation techniques.</b>
                Initially, we can obtain masks for objects of interest using the Segment Anything Model (SAM) [5]. We
                then
                customize the path distance curves for different object regions to achieve manipulated interpolation of
                anything.
            </p>
            <div class="has-text-centered">
                <img style="width: 100%;" src="./static/images/manipulation.jpg"
                     alt="Manipulation"/>
            </div>
        </div>
    </div>
</section>

<br>
<br>

<section>
    <div class="container is-max-desktop">
        <div class="content has-text-justified">
            <h2 class="title is-3">Ending</h2>
            <p>
                This work presents the next generation of video frame interpolation technology, aiming to inspire
                readers and contribute to fields like video enhancement, editing, and generation! 🔥🔥🔥
                <br>
                Welcome to &#127775; <a href="https://github.com/zzh-tech/InterpAny-Clearer">this project</a>
                and follow <a href="https://github.com/zzh-tech">the author's GitHub</a>～
            </p>
        </div>
    </div>
</section>

<br>
<hr/>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{zhong2023clearer,
    title={Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame Interpolation},
    author={Zhong, Zhihang and Krishnan, Gurunandan and Sun, Xiao and Qiao, Yu and Ma, Sizhuo and Wang, Jian},
    journal={arXiv preprint arXiv:2311.08007},
    year={2023}}</code></pre>
    </div>
</section>

<section class="section" id="acknowledgements">
    <div class="container content is-max-desktop">
        <h2 class="title">Acknowledgements</h2>
        <p>
            We thank Dorian Chan, Zhirong Wu, and Stephen Lin for their insightful feedback and advice. Our thanks also
            go to Vu An Tran for developing the web application, and to Wei Wang for coordinating the user study.
        </p>
    </div>
</section>

<section class="section" id="reference">
    <div class="container content is-max-desktop">
        <h2 class="title">Reference</h2>
        <p>
            [1] Huang, Zhewei, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. "Real-time intermediate flow
            estimation for video frame interpolation." In European Conference on Computer Vision, pp. 624-642. Cham:
            Springer Nature Switzerland, 2022.
            <br>
            [2] Kong, Lingtong, Boyuan Jiang, Donghao Luo, Wenqing Chu, Xiaoming Huang, Ying Tai, Chengjie Wang, and Jie
            Yang. "Ifrnet: Intermediate feature refine network for efficient frame interpolation." In Proceedings of the
            IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1969-1978. 2022.
            <br>
            [3] Li, Zhen, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. "AMT: All-Pairs
            Multi-Field Transforms for Efficient Frame Interpolation." In Proceedings of the IEEE/CVF Conference on
            Computer Vision and Pattern Recognition, pp. 9801-9810. 2023.
            <br>
            [4] Zhang, Guozhen, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang. "Extracting motion and
            appearance via inter-frame attention for efficient video frame interpolation." In Proceedings of the
            IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5682-5692. 2023.
            <br>
            [5] Kirillov, Alexander, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,
            Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. "Segment Anything." In
            Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4015-4026. 2023
        </p>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="content has-text-centered">
            <a class="icon-link"
               href="https://github.com/zzh-tech/InterpAny-Clearer">
                <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="https://github.com/zzh-tech" class="external-link" disabled>
                <i class="fab fa-github"></i>
            </a>
        </div>
        <div class="columns is-centered">
            <div class="column is-8">
                <div style="display: flex; justify-content: center; align-items: center; height: 100%;">
                    <p>
                        Website template from <a href="https://hypernerf.github.io/">HyperNeRF</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

<script type="text/javascript" src="./static/slick/slick.min.js"></script>
</body>
</html>
